<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning">
  <meta name="keywords" content="Promptable Manipulation, Flow, 3D, Robotics, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</title>
	<!-- link rel="icon" type="image/png" href="images/coil_icon.png"/ -->

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3E1N6NQSKY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-3E1N6NQSKY');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <!-- <div class="container is-fullhd"> -->
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><a style="color: black" href="#">Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</a></h1>

          <h3 class="title is-4 conference-authors"><a style="color: #8C1515" href="#">In Submission</a></h3>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Anonymous Authors</a>
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup> denotes equal contribution, alphabetical order</span>
          </div> -->
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Institution</span>
          </div> -->


          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- arXiv Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <!-- <video id="teaser" autoplay muted loop height="70%" width="70%">
            <source src="videos/teaser.mp4"
                    type="video/mp4">
          </video> -->
          <img src="images/teaser.jpg"
               class="interpolation-image" width="100%"
               alt="Teaser image for COIL."/>
          </br>
        </div>
        <h2 class="subtitle has-text-justified">
        We present <b>COIL</b>, a framework that utilizes <b>Spatial Correspondence</b> - the intended 3D motion of keypoints on manipulatable objects with flexible spatial and temporal granularity - as the task representation for enabling grounded robot manipulation with flexible task specification.
        </h2>
      </div>
    </div>
  </div>
</section>

<hr>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
            We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <div class="my-block">
    <div class="column is-full-width">
      <h2 class="title is-3">Spatial Correspondence Representation</h2>
      <div class="columns is-vcentered is-centered">
        <img src="images/spatial-correspondence.png"
                  class="interpolation-image"
                  alt="Spatial correspondence representation demonstration image."
                  width="70%"
                  />
      </div>
      <p class="content has-text-justified">
        We introduce a correspondence-oriented task representation to flexibly specify manipulation goals and constraints in 3D space. 
        Extending object-centric flow formulations, our task representation describes the desired changes of the environment state using a collection of K keypoints selected on the scene objects.
        As object poses and states change in the environment, the 3D coordinates of these keypoints move accordingly.
        For each keypoint k, we specify its target 3D position at H discrete steps, yielding the task representation as a tensor c.
        This tensor captures the spatio-temporal correspondences of the keypoints, constraining up to 3K degrees of freedom of the environment state at each step.
      </p>
      <p class="content has-text-justified">
        To enable flexible task specifications, we introduce key extensions that improve the expressiveness and adaptability of the correspondence-oriented specification. While prior work assume fixed keypoint counts and dense time steps, our formulation removes those rigid assumptions and enables:
      </p>
      <ul>
        <li><b>Spatial Flexibility</b>: Any K >= 1 keypoints can be selected for the task representation c to exert constraints of varying DoFs based on the task requirement. Moreover, the keypoints can be selected on multiple objects, either static or dynamic, allowing specifications of multistage tasks as well as constraints for objects that should stay still.</li>
        <li><b>Temporal Flexibility</b>: The target time steps are not restricted to fixed-length or evenly spaced intervals. We have H >= 2 target coordinates for each of the K keypoints, allowing users to provide any specifications from sparse start-goal pairs to dense flows. Instead of rigidly reaching the H target coordinates at H predetermined timesteps, we require only that the targets be reached in order, leaving execution speed and recovery behavior free to adapt online.
      </ul>
    </div>
  </div>

  <div class="my-block">
    <div class="column is-full-width">
      <h2 class="title is-3">Correspondence-Oriented Imitation Learning (COIL)</h2>
      <p class="content has-text-justified">
        With the spatial correspondence task representation, COIL learns a conditional policy that executes manipulation tasks in a self-supervised manner.
        We first collect training data by performing exploration actions in simulation and labeling the trajectory with the achieved scene 3D flows in hindsight as the spatial correspondence representation. The training data is further augmented during the training process by randomly sub-sampling keypoints and time steps to allow the policy to learn to generalize to different task specifications, and to learn to recover from failures. 
      </p>
      <div class="columns is-vcentered is-centered">
        <img src="images/main_figure.jpg"
                  class="interpolation-image"
                  alt="COIL Architecture Image."
                  width="100%"
                  />
      </div>
      <p class="content has-text-justified">
        COIL introduces the <b>Spatio-temporal Transformer</b> architecture to effectively ground sparse spatial correspondence representations into visual observations and robot actions, which interleaves self-attention layers in both spatial and temporal dimensions with cross-attention layers that fuse information from the pointcloud observations.
      </p>
    </div>
  </div>

</section>

</br>
</br>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was adapted from nerfie's <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
