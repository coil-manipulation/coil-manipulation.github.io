<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning">
  <meta name="keywords" content="Promptable Manipulation, Flow, 3D, Robotics, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</title>
	<!-- link rel="icon" type="image/png" href="images/coil_icon.png"/ -->

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3E1N6NQSKY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-3E1N6NQSKY');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <!-- <div class="container is-fullhd"> -->
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><a style="color: black" href="#">Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</a></h1>

          <h3 class="title is-4 conference-authors"><a style="color: #8C1515" href="#">In Submission</a></h3>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Anonymous Authors</a>
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup> denotes equal contribution, alphabetical order</span>
          </div> -->
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Institution</span>
          </div> -->


          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- arXiv Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <!-- <video id="teaser" autoplay muted loop height="70%" width="70%">
            <source src="videos/teaser.mp4"
                    type="video/mp4">
          </video> -->
          <img src="images/teaser.jpg"
               class="interpolation-image" width="100%"
               alt="Teaser image for COIL."/>
          </br>
        </div>
        <h2 class="subtitle has-text-justified">
        We present <b>COIL</b>, a framework that utilizes <b>Spatial Correspondence</b> - the intended 3D motion of keypoints on manipulatable objects with flexible spatial and temporal granularity - as the task representation for enabling grounded robot manipulation with flexible task specification.
        </h2>
      </div>
    </div>
  </div>
</section>

<hr>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
            We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <div class="my-block">
    <div class="column is-full-width">
      <h2 class="title is-3">Spatial Correspondence Representation</h2>
      <div class="columns is-vcentered is-centered">
        <img src="images/spatial-correspondence.png"
                  class="interpolation-image"
                  alt="Spatial correspondence representation demonstration image."
                  width="70%"
                  />
      </div>
      <p class="content has-text-justified">
        Extending object-centric flow formulations, our task representation describes the desired changes of the environment state using a collection of K keypoints selected on the scene objects, spanning across H discrete time steps. By allowing varying number of keypoints and time steps, we achieve the following flexibilities:
      </p>
      <ul>
        <li><b>Spatial Flexibility</b>: Any K >= 1 keypoints can be selected to exert constraints of varying DoFs on multiple objects.</li>
        <li><b>Temporal Flexibility</b>: We have H >= 2 target coordinates for each of the K keypoints, allowing users to provide any specifications from sparse start-goal pairs to dense flows. </li>
      </ul>
    </div>
  </div>

  <div class="my-block">
    <div class="column is-full-width">
      <h2 class="title is-3">Correspondence-Oriented Imitation Learning (COIL)</h2>
      <!-- <p class="content has-text-justified">
        With the spatial correspondence task representation, COIL learns a conditional policy that executes manipulation tasks in a self-supervised manner.
        We first collect training data by performing exploration actions in simulation and labeling the trajectory with the achieved scene 3D flows in hindsight as the spatial correspondence representation. The training data is further augmented during the training process by randomly sub-sampling keypoints and time steps to allow the policy to learn to generalize to different task specifications, and to learn to recover from failures. 
      </p> -->
      <div class="columns is-vcentered is-centered">
        <img src="images/main_figure.jpg"
                  class="interpolation-image"
                  alt="COIL Architecture Image."
                  width="100%"
                  />
      </div>
      <p class="content has-text-justified">
        COIL introduces the <b>Spatio-temporal Transformer</b> architecture to effectively ground sparse spatial correspondence representations into visual observations and robot actions, which interleaves self-attention layers in both spatial and temporal dimensions with cross-attention layers that fuse information from the pointcloud observations.
      </p>
    </div>
  </div>
  
  <div class="my-block">
    <div class="rows is-fullhd">
      <h2 class="title is-3">Tasks</h2>
      <p class="content has-text-justified">
        Given both sparse and dense task specifications, COIL is able to generalize to a variety of manipulation tasks with novel object appearances and shapes. Below are some example tasks that COIL can perform.
      </p>
      
      <div class="rows">
        <div class="columns">
          <div class="column has-text-justified">
            <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/pick-place-1.mp4" type="video/mp4">
            </video>
          </div>
      
          <div class="column has-text-centered">
            <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/pick-place-2.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/sweep-carrot-1.mp4" type="video/mp4">
            </video>
          </div>
      
          <div class="column has-text-centered">
            <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/sweep-carrot-2.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      <div class="rows">
        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/fold-cloth-1.mp4" type="video/mp4">
            </video>
          </div>
      
          <div class="column has-text-centered">
            <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/fold-cloth-2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>

  <br />

  <div class="my-block">
    <div class="rows is-fullhd">
      <h2 class="title is-3">Additional Tasks</h2>
      <p class="content has-text-justified">
        We further demonstrate the flexibility of COIL by performing tasks with both sparse and dense specifications on a pick-place-avoidance task and a wiping task.
      </p>
      
      <div class="rows">
        <div class="columns">
          <div class="column has-text-centered">
            <video id="dist1" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/pick-apple.mp4" type="video/mp4">
            </video>
          </div>
      
          <div class="column has-text-centered">
            <video id="dist2" controls="" muted="" autoplay="" loop="" width="99%">
              <source src="videos/wipe-table.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>

  
</section>

</br>
</br>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was adapted from nerfie's <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
